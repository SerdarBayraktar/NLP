{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-28T15:14:48.156723Z",
     "start_time": "2024-09-28T15:14:48.154976Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "import openai\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "import json\n",
    "import ollama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "sys.path.append('../..')\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "#openai.api_key  = os.environ['OPENAI_API_KEY']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T15:13:45.697378Z",
     "start_time": "2024-09-28T15:13:45.689595Z"
    }
   },
   "id": "62eb27426eb345ee"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "loaders = [\n",
    "    # Duplicate documents on purpose - messy data\n",
    "    PyPDFLoader(\"pdf/book.pdf\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T15:13:48.423846Z",
     "start_time": "2024-09-28T15:13:47.119336Z"
    }
   },
   "id": "3ce61227efb2672"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T15:27:09.050892Z",
     "start_time": "2024-09-28T15:27:09.049016Z"
    }
   },
   "id": "df0f3461a1475565"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Loading egg at /Users/ivy/anaconda3/lib/python3.11/site-packages/binwalk-2.3.3-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: langchain-openai in /Users/ivy/anaconda3/lib/python3.11/site-packages (0.2.1)\r\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-openai) (0.3.6)\r\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-openai) (1.50.1)\r\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-openai) (0.7.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (6.0.1)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (1.33)\r\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (0.1.128)\r\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (23.2)\r\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (2.9.2)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (8.2.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langchain-core<0.4,>=0.3->langchain-openai) (4.11.0)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.2.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.8.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.27.0)\r\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.5.0)\r\n",
      "Requirement already satisfied: sniffio in /Users/ivy/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.0)\r\n",
      "Requirement already satisfied: tqdm>4 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.65.0)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2023.10.3)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.31.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (3.4)\r\n",
      "Requirement already satisfied: certifi in /Users/ivy/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (2024.8.30)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ivy/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.0.5)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (0.14.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain-openai) (2.1)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-openai) (3.10.3)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-openai) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3->langchain-openai) (2.23.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ivy/anaconda3/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.7)\r\n"
     ]
    }
   ],
   "source": [
    "#!pip install langchain-openai"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T10:53:14.613053Z",
     "start_time": "2024-09-28T10:53:11.760352Z"
    }
   },
   "id": "d75195840fddd3be"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#embedding = OpenAIEmbeddings()\n",
    "# persist_directory = 'docs/chroma/'\n",
    "# !rm -rf ./docs/chroma  # remove old database files if any\n",
    "# vectordb = Chroma.from_documents(\n",
    "#     documents=splits,\n",
    "#     embedding=embedding,\n",
    "#     persist_directory=persist_directory\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T10:53:19.047044Z",
     "start_time": "2024-09-28T10:53:18.999928Z"
    }
   },
   "id": "bb1e492bbaa17e40"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import ollama\n",
    "embedding = ollama.embeddings(\n",
    "  model='mxbai-embed-large',# also nomic-embed-text avilable\n",
    "  prompt='Llamas are members of the camelid family',\n",
    ")\n",
    "persist_directory = 'docs/chroma/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T15:15:08.687207Z",
     "start_time": "2024-09-28T15:15:07.301320Z"
    }
   },
   "id": "a37ae434f7c67b45"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "!rm -rf ./docs/chroma  # remove old database files if any"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T15:16:18.045480Z",
     "start_time": "2024-09-28T15:16:17.915667Z"
    }
   },
   "id": "b195b8b12b5fa4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"docs\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7143cc1ebf1018f"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "'help in that regard. It provides a road map for getting the product-market ï¬t as tight as possible. There\\nare many considerations in this process and this book captures them well and provides practical\\nguidance on how to resolve them. â€\\nâ€”Tom Byers, Entrepreneurship Professorship Endowed Chair in the\\nStanford School of Engineering; Faculty Director, Stanford Technology Ventures Program\\nâ€œThis is an excellent practical guide for entrepreneurs so they can see the whole process and not miss\\ncritical steps as they bring products to market. Growing out of the actual experience of teaching MITstudents, it adds to the growing body of thoughtful literature in the ï¬eld that bodes well for the\\nconsistent development of young entrepreneurs.â€\\nâ€”Joe Lassiter, Faculty Chair of the Harvard Innovation Lab,\\nand Heinz Professor of Management Practice at the Harvard Business School'"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[5].page_content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T15:29:56.796667Z",
     "start_time": "2024-09-28T15:29:56.783817Z"
    }
   },
   "id": "a401f277bc08053d"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 0\n",
      "Processing document 1\n",
      "Processing document 2\n",
      "Processing document 3\n",
      "Processing document 4\n",
      "Processing document 5\n",
      "Processing document 6\n",
      "Processing document 7\n",
      "Processing document 8\n",
      "Processing document 9\n",
      "Processing document 10\n",
      "Processing document 11\n",
      "Processing document 12\n",
      "Processing document 13\n",
      "Processing document 14\n",
      "Processing document 15\n",
      "Processing document 16\n",
      "Processing document 17\n",
      "Processing document 18\n",
      "Processing document 19\n",
      "Processing document 20\n",
      "Processing document 21\n",
      "Processing document 22\n",
      "Processing document 23\n",
      "Processing document 24\n",
      "Processing document 25\n",
      "Processing document 26\n",
      "Processing document 27\n",
      "Processing document 28\n",
      "Processing document 29\n",
      "Processing document 30\n",
      "Processing document 31\n",
      "Processing document 32\n",
      "Processing document 33\n",
      "Processing document 34\n",
      "Processing document 35\n",
      "Processing document 36\n",
      "Processing document 37\n",
      "Processing document 38\n",
      "Processing document 39\n",
      "Processing document 40\n",
      "Processing document 41\n",
      "Processing document 42\n",
      "Processing document 43\n",
      "Processing document 44\n",
      "Processing document 45\n",
      "Processing document 46\n",
      "Processing document 47\n",
      "Processing document 48\n",
      "Processing document 49\n",
      "Processing document 50\n",
      "Processing document 51\n",
      "Processing document 52\n",
      "Processing document 53\n",
      "Processing document 54\n",
      "Processing document 55\n",
      "Processing document 56\n",
      "Processing document 57\n",
      "Processing document 58\n",
      "Processing document 59\n",
      "Processing document 60\n",
      "Processing document 61\n",
      "Processing document 62\n",
      "Processing document 63\n",
      "Processing document 64\n",
      "Processing document 65\n",
      "Processing document 66\n",
      "Processing document 67\n",
      "Processing document 68\n",
      "Processing document 69\n",
      "Processing document 70\n",
      "Processing document 71\n",
      "Processing document 72\n",
      "Processing document 73\n",
      "Processing document 74\n",
      "Processing document 75\n",
      "Processing document 76\n",
      "Processing document 77\n",
      "Processing document 78\n",
      "Processing document 79\n",
      "Processing document 80\n",
      "Processing document 81\n",
      "Processing document 82\n",
      "Processing document 83\n",
      "Processing document 84\n",
      "Processing document 85\n",
      "Processing document 86\n",
      "Processing document 87\n",
      "Processing document 88\n",
      "Processing document 89\n",
      "Processing document 90\n",
      "Processing document 91\n",
      "Processing document 92\n",
      "Processing document 93\n",
      "Processing document 94\n",
      "Processing document 95\n",
      "Processing document 96\n",
      "Processing document 97\n",
      "Processing document 98\n",
      "Processing document 99\n",
      "Processing document 100\n",
      "Processing document 101\n",
      "Processing document 102\n",
      "Processing document 103\n",
      "Processing document 104\n",
      "Processing document 105\n",
      "Processing document 106\n",
      "Processing document 107\n",
      "Processing document 108\n",
      "Processing document 109\n",
      "Processing document 110\n",
      "Processing document 111\n",
      "Processing document 112\n",
      "Processing document 113\n",
      "Processing document 114\n",
      "Processing document 115\n",
      "Processing document 116\n",
      "Processing document 117\n",
      "Processing document 118\n",
      "Processing document 119\n",
      "Processing document 120\n",
      "Processing document 121\n",
      "Processing document 122\n",
      "Processing document 123\n",
      "Processing document 124\n",
      "Processing document 125\n",
      "Processing document 126\n",
      "Processing document 127\n",
      "Processing document 128\n",
      "Processing document 129\n",
      "Processing document 130\n",
      "Processing document 131\n",
      "Processing document 132\n",
      "Processing document 133\n",
      "Processing document 134\n",
      "Processing document 135\n",
      "Processing document 136\n",
      "Processing document 137\n",
      "Processing document 138\n",
      "Processing document 139\n",
      "Processing document 140\n",
      "Processing document 141\n",
      "Processing document 142\n",
      "Processing document 143\n",
      "Processing document 144\n",
      "Processing document 145\n",
      "Processing document 146\n",
      "Processing document 147\n",
      "Processing document 148\n",
      "Processing document 149\n",
      "Processing document 150\n",
      "Processing document 151\n",
      "Processing document 152\n",
      "Processing document 153\n",
      "Processing document 154\n",
      "Processing document 155\n",
      "Processing document 156\n",
      "Processing document 157\n",
      "Processing document 158\n",
      "Processing document 159\n",
      "Processing document 160\n",
      "Processing document 161\n",
      "Processing document 162\n",
      "Processing document 163\n",
      "Processing document 164\n",
      "Processing document 165\n",
      "Processing document 166\n",
      "Processing document 167\n",
      "Processing document 168\n",
      "Processing document 169\n",
      "Processing document 170\n",
      "Processing document 171\n",
      "Processing document 172\n",
      "Processing document 173\n",
      "Processing document 174\n",
      "Processing document 175\n",
      "Processing document 176\n",
      "Processing document 177\n",
      "Processing document 178\n",
      "Processing document 179\n",
      "Processing document 180\n",
      "Processing document 181\n",
      "Processing document 182\n",
      "Processing document 183\n",
      "Processing document 184\n",
      "Processing document 185\n",
      "Processing document 186\n",
      "Processing document 187\n",
      "Processing document 188\n",
      "Processing document 189\n",
      "Processing document 190\n",
      "Processing document 191\n",
      "Processing document 192\n",
      "Processing document 193\n",
      "Processing document 194\n",
      "Processing document 195\n",
      "Processing document 196\n",
      "Processing document 197\n",
      "Processing document 198\n",
      "Processing document 199\n",
      "Processing document 200\n",
      "Processing document 201\n",
      "Processing document 202\n",
      "Processing document 203\n",
      "Processing document 204\n",
      "Processing document 205\n",
      "Processing document 206\n",
      "Processing document 207\n",
      "Processing document 208\n",
      "Processing document 209\n",
      "Processing document 210\n",
      "Processing document 211\n",
      "Processing document 212\n",
      "Processing document 213\n",
      "Processing document 214\n",
      "Processing document 215\n",
      "Processing document 216\n",
      "Processing document 217\n",
      "Processing document 218\n",
      "Processing document 219\n",
      "Processing document 220\n",
      "Processing document 221\n",
      "Processing document 222\n",
      "Processing document 223\n",
      "Processing document 224\n",
      "Processing document 225\n",
      "Processing document 226\n",
      "Processing document 227\n",
      "Processing document 228\n",
      "Processing document 229\n",
      "Processing document 230\n",
      "Processing document 231\n",
      "Processing document 232\n",
      "Processing document 233\n",
      "Processing document 234\n",
      "Processing document 235\n",
      "Processing document 236\n",
      "Processing document 237\n",
      "Processing document 238\n",
      "Processing document 239\n",
      "Processing document 240\n",
      "Processing document 241\n",
      "Processing document 242\n",
      "Processing document 243\n",
      "Processing document 244\n",
      "Processing document 245\n",
      "Processing document 246\n",
      "Processing document 247\n",
      "Processing document 248\n",
      "Processing document 249\n",
      "Processing document 250\n",
      "Processing document 251\n",
      "Processing document 252\n",
      "Processing document 253\n",
      "Processing document 254\n",
      "Processing document 255\n",
      "Processing document 256\n",
      "Processing document 257\n",
      "Processing document 258\n",
      "Processing document 259\n",
      "Processing document 260\n",
      "Processing document 261\n",
      "Processing document 262\n",
      "Processing document 263\n",
      "Processing document 264\n",
      "Processing document 265\n",
      "Processing document 266\n",
      "Processing document 267\n",
      "Processing document 268\n",
      "Processing document 269\n",
      "Processing document 270\n",
      "Processing document 271\n",
      "Processing document 272\n",
      "Processing document 273\n",
      "Processing document 274\n",
      "Processing document 275\n",
      "Processing document 276\n",
      "Processing document 277\n",
      "Processing document 278\n",
      "Processing document 279\n",
      "Processing document 280\n",
      "Processing document 281\n",
      "Processing document 282\n",
      "Processing document 283\n",
      "Processing document 284\n",
      "Processing document 285\n",
      "Processing document 286\n",
      "Processing document 287\n",
      "Processing document 288\n",
      "Processing document 289\n",
      "Processing document 290\n",
      "Processing document 291\n",
      "Processing document 292\n",
      "Processing document 293\n",
      "Processing document 294\n",
      "Processing document 295\n",
      "Processing document 296\n",
      "Processing document 297\n",
      "Processing document 298\n",
      "Processing document 299\n",
      "Processing document 300\n",
      "Processing document 301\n",
      "Processing document 302\n",
      "Processing document 303\n",
      "Processing document 304\n",
      "Processing document 305\n",
      "Processing document 306\n",
      "Processing document 307\n",
      "Processing document 308\n",
      "Processing document 309\n",
      "Processing document 310\n",
      "Processing document 311\n",
      "Processing document 312\n",
      "Processing document 313\n",
      "Processing document 314\n",
      "Processing document 315\n",
      "Processing document 316\n",
      "Processing document 317\n",
      "Processing document 318\n",
      "Processing document 319\n",
      "Processing document 320\n",
      "Processing document 321\n",
      "Processing document 322\n",
      "Processing document 323\n",
      "Processing document 324\n",
      "Processing document 325\n",
      "Processing document 326\n",
      "Processing document 327\n",
      "Processing document 328\n",
      "Processing document 329\n",
      "Processing document 330\n",
      "Processing document 331\n",
      "Processing document 332\n",
      "Processing document 333\n",
      "Processing document 334\n",
      "Processing document 335\n",
      "Processing document 336\n",
      "Processing document 337\n",
      "Processing document 338\n",
      "Processing document 339\n",
      "Processing document 340\n",
      "Processing document 341\n",
      "Processing document 342\n",
      "Processing document 343\n",
      "Processing document 344\n",
      "Processing document 345\n",
      "Processing document 346\n",
      "Processing document 347\n",
      "Processing document 348\n",
      "Processing document 349\n",
      "Processing document 350\n",
      "Processing document 351\n",
      "Processing document 352\n",
      "Processing document 353\n",
      "Processing document 354\n",
      "Processing document 355\n",
      "Processing document 356\n",
      "Processing document 357\n",
      "Processing document 358\n",
      "Processing document 359\n",
      "Processing document 360\n",
      "Processing document 361\n",
      "Processing document 362\n",
      "Processing document 363\n",
      "Processing document 364\n",
      "Processing document 365\n",
      "Processing document 366\n",
      "Processing document 367\n",
      "Processing document 368\n",
      "Processing document 369\n",
      "Processing document 370\n",
      "Processing document 371\n",
      "Processing document 372\n",
      "Processing document 373\n",
      "Processing document 374\n",
      "Processing document 375\n",
      "Processing document 376\n",
      "Processing document 377\n",
      "Processing document 378\n",
      "Processing document 379\n",
      "Processing document 380\n",
      "Processing document 381\n",
      "Processing document 382\n",
      "Processing document 383\n",
      "Processing document 384\n",
      "Processing document 385\n",
      "Processing document 386\n",
      "Processing document 387\n",
      "Processing document 388\n",
      "Processing document 389\n",
      "Processing document 390\n",
      "Processing document 391\n",
      "Processing document 392\n",
      "Processing document 393\n",
      "Processing document 394\n",
      "Processing document 395\n",
      "Processing document 396\n",
      "Processing document 397\n",
      "Processing document 398\n",
      "Processing document 399\n",
      "Processing document 400\n",
      "Processing document 401\n",
      "Processing document 402\n",
      "Processing document 403\n",
      "Processing document 404\n",
      "Processing document 405\n",
      "Processing document 406\n",
      "Processing document 407\n",
      "Processing document 408\n",
      "Processing document 409\n",
      "Processing document 410\n",
      "Processing document 411\n",
      "Processing document 412\n",
      "Processing document 413\n",
      "Processing document 414\n",
      "Processing document 415\n",
      "Processing document 416\n",
      "Processing document 417\n",
      "Processing document 418\n",
      "Processing document 419\n",
      "Processing document 420\n",
      "Processing document 421\n",
      "Processing document 422\n",
      "Processing document 423\n",
      "Processing document 424\n",
      "Processing document 425\n",
      "Processing document 426\n",
      "Processing document 427\n",
      "Processing document 428\n",
      "Processing document 429\n",
      "Processing document 430\n",
      "Processing document 431\n",
      "Processing document 432\n",
      "Processing document 433\n",
      "Processing document 434\n",
      "Processing document 435\n",
      "Processing document 436\n",
      "Processing document 437\n",
      "Processing document 438\n",
      "Processing document 439\n",
      "Processing document 440\n",
      "Processing document 441\n",
      "Processing document 442\n",
      "Processing document 443\n",
      "Processing document 444\n",
      "Processing document 445\n",
      "Processing document 446\n",
      "Processing document 447\n",
      "Processing document 448\n",
      "Processing document 449\n",
      "Processing document 450\n",
      "Processing document 451\n",
      "Processing document 452\n",
      "Processing document 453\n",
      "Processing document 454\n",
      "Processing document 455\n",
      "Processing document 456\n",
      "Processing document 457\n",
      "Processing document 458\n",
      "Processing document 459\n",
      "Processing document 460\n",
      "Processing document 461\n",
      "Processing document 462\n",
      "Processing document 463\n",
      "Processing document 464\n",
      "Processing document 465\n",
      "Processing document 466\n",
      "Processing document 467\n",
      "Processing document 468\n",
      "Processing document 469\n",
      "Processing document 470\n",
      "Processing document 471\n",
      "Processing document 472\n",
      "Processing document 473\n"
     ]
    }
   ],
   "source": [
    "# store each document in a vector embedding database\n",
    "for i, d in enumerate(splits):\n",
    "    page_content = str(d.page_content)    \n",
    "    try:\n",
    "        response = ollama.embeddings(\n",
    "            model=\"mxbai-embed-large\", \n",
    "            prompt=page_content\n",
    "        )\n",
    "        embedding = response[\"embedding\"]\n",
    "        collection.add(\n",
    "            ids=[str(i)],\n",
    "            embeddings=[embedding],\n",
    "            documents=[page_content], \n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {i}: {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T22:23:54.611737Z",
     "start_time": "2024-09-28T22:23:13.656877Z"
    }
   },
   "id": "5e761063db38e7ee"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "'online males between the ages of 18 and 34. This segment was a very attractive group to capture,\\nbecause liquor, automotive, and electronics companies were anxious to advertise to this demographic\\nas they started to earn signi ï¬cant incomes and form buying habits that might last a lifetime. The team\\ndid its primary market research and found there was receptivity. They also used some secondaryresearch to learn more about their demographic.\\nFigure 3.1 shows a key component of their End User Pro ï¬le.\\nThe charts presented in the ï¬gure indicate that they were able to use certain metrics to narrow\\ntheir focus to 25-to-34-year-old males making over $75K per year. Implicit in this choice is the team\\nwill be deselecting the other demographics and pursuing only one demographic to start. The criteria\\nofâ€œtarget audience interests â€is a good example of needing to confront the brutally honest facts about\\nyour demographic, rather than looking at the end user through rose-colored glasses. From the sec-ondary research presented here, the two top website choices for the broader 18 â€“34 age demographic\\nare sites to look at girls (we hope nice sites â€”but we will have to ï¬nd out the reality to truly\\nunderstand the end user) and sports sites.Build an End User Pro ï¬le 55'"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Describe a persona about the market segment adults between 20 and 25?\"\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "docs[0].page_content"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T22:24:28.160650Z",
     "start_time": "2024-09-28T22:24:27.632850Z"
    }
   },
   "id": "6da94396a5d339bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LLM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8933a467b9ab54f0"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "### LLM\n",
    "local_llm = 'llama3.2:1b-instruct-fp16'\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "### Router\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T22:24:35.569602Z",
     "start_time": "2024-09-28T22:24:35.524764Z"
    }
   },
   "id": "da43842b65fb485f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test():\n",
    "    llm_json_mode = ChatOllama(model=local_llm, temperature=0, format='json')\n",
    "    # sil bunu\n",
    "    # document_content_description = \"Lecture notes\"\n",
    "    # metadata_field_info = [\n",
    "    #     AttributeInfo(\n",
    "    #         name=\"source\",\n",
    "    #         description=\"The lecture the chunk is from, should be one of `docs/book.pdf`\",\n",
    "    #         type=\"string\",\n",
    "    #     ),\n",
    "    # ]\n",
    "    # Prompt \n",
    "    router_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "    \n",
    "    The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "                                        \n",
    "    Use the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n",
    "    \n",
    "    Return JSON with single key, datasource, that is 'websearch' or 'vectorstore' depending on the question.\"\"\"\n",
    "    \n",
    "    # Test router\n",
    "    test_web_search = llm_json_mode.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=\"Who is favored to win the NFC Championship game in the 2024 season?\")])\n",
    "    test_web_search_2 = llm_json_mode.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=\"What are the models released today for llama3.2?\")])\n",
    "    test_vector_store = llm_json_mode.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=\"What are the types of agent memory?\")])\n",
    "    print(json.loads(test_web_search.content), json.loads(test_web_search_2.content), json.loads(test_vector_store.content))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da0a16077f9a15cd"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "# create chain \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T22:27:03.571260Z",
     "start_time": "2024-09-28T22:27:03.563886Z"
    }
   },
   "id": "82621da54159746e"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "'According to the text:\\n\\n* A beachhead market is one of the critical steps in the process of narrowing down a focus and attention to one specific area of attack.\\n* It involves selecting a top 1-12 market opportunity from your initial list, further segmenting that market to determine the \"beachhead market\", which is the most promising and profitable market.\\n\\nThis step is Step 2.'"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test 1\n",
    "question = \"what is beachead market? and which step is it in?\"\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T22:27:08.138482Z",
     "start_time": "2024-09-28T22:27:05.191562Z"
    }
   },
   "id": "287f6d7a77c1b9a8"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# prepate template\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"],template=template,)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T22:25:00.907612Z",
     "start_time": "2024-09-28T22:25:00.905022Z"
    }
   },
   "id": "560586f01347a21d"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "'thanks for asking! \\n\\nA selectable market refers to the first market that you can realistically and efficiently enter with your existing resources, without having to wait for a larger or more established market to mature. It\\'s the initial market where you can gain high exposure among potential customers and test your product or service before expanding into other markets.\\n\\nIn other words, it\\'s the \"first shot\" at success in a new market, rather than trying to enter a large or well-established market that may take time to develop.'"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test template\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "question = 'what is selectable market in step 1'\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T22:25:03.756942Z",
     "start_time": "2024-09-28T22:25:00.907740Z"
    }
   },
   "id": "d1326df0e6381a80"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "Document(metadata={'page': 58, 'source': 'pdf/book.pdf'}, page_content='STEP 2\\nSelect a Beachhead Market\\n41')"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show where it is \n",
    "result[\"source_documents\"][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T22:25:03.757249Z",
     "start_time": "2024-09-28T22:25:03.754583Z"
    }
   },
   "id": "e9ba3540b6966ac0"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "\"I don't know, thanks for asking!\""
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test but with info. \n",
    "# Rag doesnt work here\n",
    "# template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "# {context}\n",
    "# Question: {question}\n",
    "# Helpful Answer:\"\"\"\n",
    "# question = 'what is beachead market?'\n",
    "# qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "#                                        retriever=vectordb.as_retriever(),\n",
    "#                                        return_source_documents=True,\n",
    "#                                        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT})\n",
    "# \n",
    "# \n",
    "# result = qa_chain({\"query\": question})\n",
    "# result[\"result\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T22:26:02.243661Z",
     "start_time": "2024-09-28T22:26:00.080371Z"
    }
   },
   "id": "cb6a8adf73e0d86"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A beachhead market refers to a position or state where a business has established itself with positive cash flow before it runs out, allowing for quick achievement of positive word of mouth (WOM) that can be a source of success or failure. In military operations, a beachhead strategy involves establishing a base of operations in enemy territory to launch further attacks and capture adjacent areas. For entrepreneurs, identifying a beachhead market is crucial as it enables them to establish a strong foundation for future growth and expansion.\n"
     ]
    }
   ],
   "source": [
    "### Generate this is good.\n",
    "\n",
    "# Prompt\n",
    "rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "\n",
    "Here is the context to use to answer the question:\n",
    "\n",
    "{context} \n",
    "\n",
    "Think carefully about the above context. \n",
    "\n",
    "Now, review the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Provide an answer to this questions using only the above context. \n",
    "\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Test\n",
    "docs = vectordb.as_retriever().invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "print(generation.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T22:26:13.732581Z",
     "start_time": "2024-09-28T22:26:10.279829Z"
    }
   },
   "id": "2a4e746111597caf"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This agent relies on access to a python repl tool which can execute arbitrary code. This can be dangerous and requires a specially sandboxed environment to be safely used. Please read the security notice in the doc-string of this function. You must opt-in to use this functionality by setting allow_dangerous_code=True.For general security guidelines, please see: https://python.langchain.com/v0.2/docs/security/",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[37], line 22\u001B[0m\n\u001B[1;32m     13\u001B[0m uploaded_file \u001B[38;5;241m=\u001B[39m st\u001B[38;5;241m.\u001B[39mfile_uploader(\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpload a Data file\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlist\u001B[39m(file_formats\u001B[38;5;241m.\u001B[39mkeys()),\n\u001B[1;32m     16\u001B[0m     help\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVarious File formats are Support\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;66;03m#on_change=clear_submit,\u001B[39;00m\n\u001B[1;32m     18\u001B[0m )\n\u001B[1;32m     20\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 22\u001B[0m pandas_df_agent \u001B[38;5;241m=\u001B[39m create_pandas_dataframe_agent(\n\u001B[1;32m     23\u001B[0m     llm,\n\u001B[1;32m     24\u001B[0m     df,\n\u001B[1;32m     25\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     26\u001B[0m     agent_type\u001B[38;5;241m=\u001B[39mAgentType\u001B[38;5;241m.\u001B[39mOPENAI_FUNCTIONS,\n\u001B[1;32m     27\u001B[0m     handle_parsing_errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     28\u001B[0m )\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m st\u001B[38;5;241m.\u001B[39mchat_message(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124massistant\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     31\u001B[0m     st_cb \u001B[38;5;241m=\u001B[39m StreamlitCallbackHandler(st\u001B[38;5;241m.\u001B[39mcontainer(), expand_new_thoughts\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_experimental/agents/agent_toolkits/pandas/base.py:249\u001B[0m, in \u001B[0;36mcreate_pandas_dataframe_agent\u001B[0;34m(llm, df, agent_type, callback_manager, prefix, suffix, input_variables, verbose, return_intermediate_steps, max_iterations, max_execution_time, early_stopping_method, agent_executor_kwargs, include_df_in_prompt, number_of_head_rows, extra_tools, engine, allow_dangerous_code, **kwargs)\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Construct a Pandas agent from an LLM and dataframe(s).\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \n\u001B[1;32m    177\u001B[0m \u001B[38;5;124;03mSecurity Notice:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    246\u001B[0m \n\u001B[1;32m    247\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_dangerous_code:\n\u001B[0;32m--> 249\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    250\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis agent relies on access to a python repl tool which can execute \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    251\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marbitrary code. This can be dangerous and requires a specially sandboxed \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    252\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menvironment to be safely used. Please read the security notice in the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    253\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdoc-string of this function. You must opt-in to use this functionality \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    254\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mby setting allow_dangerous_code=True.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    255\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFor general security guidelines, please see: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    256\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://python.langchain.com/v0.2/docs/security/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    257\u001B[0m     )\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m engine \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodin\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[0;31mValueError\u001B[0m: This agent relies on access to a python repl tool which can execute arbitrary code. This can be dangerous and requires a specially sandboxed environment to be safely used. Please read the security notice in the doc-string of this function. You must opt-in to use this functionality by setting allow_dangerous_code=True.For general security guidelines, please see: https://python.langchain.com/v0.2/docs/security/"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentType\n",
    "from langchain_experimental.agents import create_pandas_dataframe_agent\n",
    "from langchain.callbacks import StreamlitCallbackHandler\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import os\n",
    "file_formats = {\n",
    "    \"csv\": pd.read_csv,\n",
    "}\n",
    "st.set_page_config(page_title=\"LangChain: Chat with Pandas DataFrame\", page_icon=\"ðŸ¦œ\")\n",
    "st.title(\"ðŸ¦œ LangChain: Chat with Pandas DataFrame\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\n",
    "    \"Upload a Data file\",\n",
    "    type=list(file_formats.keys()),\n",
    "    help=\"Various File formats are Support\",\n",
    "    #on_change=clear_submit,\n",
    ")\n",
    "\n",
    "df = pd.read_csv('test.csv')\n",
    "\n",
    "pandas_df_agent = create_pandas_dataframe_agent(\n",
    "    llm,\n",
    "    df,\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "    \n",
    "with st.chat_message(\"assistant\"):\n",
    "    st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)\n",
    "    response = pandas_df_agent.run(st.session_state.messages, callbacks=[st_cb])\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    st.write(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-27T23:31:10.660713Z",
     "start_time": "2024-09-27T23:31:10.586843Z"
    }
   },
   "id": "b68e93aa7f0eb8af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "91f3b169a756dd53"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
